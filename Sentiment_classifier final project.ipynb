{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XNqCW7REKMK",
        "outputId": "e682abe3-9444-415d-b57c-92ecd1c861b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JJ2AenGe22H",
        "outputId": "a37003a3-10e7-45ab-cf5d-58ab48703ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfP0GiNSdyiz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cw8GRvneAeK"
      },
      "outputs": [],
      "source": [
        "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index     # ignore unrelevent padding infomation\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE_MODEL = 1e-5             # model main part learning rate\n",
        "LEARNING_RATE_CLASSIFIER = 1e-3        # for the classifier at the top of the model\n",
        "WARMUP_STEPS = 0                       # defines the number of steps for gradually increasing the learning rate at the beginning of training. Here, set to 0, it means no warmup is used.\n",
        "                                       #Warmup can help stabilize the early phase of training, preventing the model from diverging due to a high learning rate initially.\n",
        "GRADIENT_ACCUMULATION_STEPS = 1        #Gradient accumulation allows for simulating larger batches in smaller memory by accumulating gradients over multiple smaller batches. here set to 1, meaning no gradient accumulation is used.\n",
        "MAX_GRAD_NORM = 1.0                     #This is the threshold for gradient clipping. Gradient clipping is used to prevent the exploding gradients\n",
        "SEED = 42\n",
        "NO_CUDA = False                          #This flag indicates whether to use CUDA (i.e., GPU acceleration).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPvYattqeC01"
      },
      "outputs": [],
      "source": [
        "#rpad function To ensure the consistency of sequence lengths.\n",
        "#If a sequence is longer than the specified length (here 256), it is truncated; if shorter, it is padded with zeros to the specified length.\n",
        "\n",
        "def rpad(array, n):\n",
        "    current_len = len(array)\n",
        "    if current_len > n:\n",
        "        return array[:n]\n",
        "    extra = n - current_len\n",
        "    return array + ([0] * extra)\n",
        "\n",
        "# To convert text data into a format suitable for processing by the BERT model.\n",
        "# It tokenizes the text using a tokenizer (e.g., BertTokenizer), converts tokens into IDs understandable by the model,\n",
        "# and adds special tokens [CLS] and [SEP] at the beginning and end of the sequence.\n",
        "#tokens = tokens[:250] means that the list of tokens, after tokenization,\n",
        "#is truncated to retain only the first 250 elements. This is typically done to ensure consistency and efficiency in the model's input.\n",
        "\n",
        "def convert_to_embedding(tokenizer, sentences_with_labels):\n",
        "    for sentence, label in sentences_with_labels:\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        tokens = tokens[:250]\n",
        "        bert_sent = rpad(tokenizer.convert_tokens_to_ids([\"CLS\"] + tokens + [\"SEP\"]), n=256)          #encoding，add ID and add special sign\n",
        "        yield torch.tensor(bert_sent), torch.tensor(label, dtype=torch.int64)                       # data is converted into PyTorch tensors, ready to be fed into the model.\n",
        "\n",
        "\n",
        "\n",
        "# parse line function is for Text Cleaning，To clean and format text, such as removing HTML tags, converting to lowercase,\n",
        "def parse_line(line):\n",
        "    line = line.strip().lower()\n",
        "    line = line.replace(\"&nbsp;\", \" \")\n",
        "    line = re.sub(r'<br(\\s\\/)?>', ' ', line)\n",
        "    line = re.sub(r' +', ' ', line)  # merge multiple spaces into one\n",
        "\n",
        "    return line\n",
        "\n",
        "#To read text data from a file, applying the parse_line function to clean each line.\n",
        "def read_imdb_data(filename):\n",
        "    data = []\n",
        "    for line in open(filename, 'r', encoding=\"utf-8\"):\n",
        "        data.append(parse_line(line))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def prepare_dataloader(tokenizer, sampler=RandomSampler, train=False):\n",
        "    filename = \"/content/gdrive/My Drive/bert/imdb_train.txt\" if train else \"/content/gdrive/My Drive/bert/imdb_test.txt\"\n",
        "\n",
        "    data = read_imdb_data(filename)\n",
        "    y = np.append(np.zeros(12500), np.ones(12500)) #creates an array of labels where the first 12500 labels are 0 (negative reviews), the next 12500 labels are 1 (positive reviews).\n",
        "    sentences_with_labels = zip(data, y.tolist())  #combines the text data and corresponding labels into pairs.\n",
        "\n",
        "    dataset = list(convert_to_embedding(tokenizer, sentences_with_labels))  #converts the paired text and labels into a format that can be understood by the model\n",
        "\n",
        "    sampler_func = sampler(dataset) if sampler is not None else None\n",
        "    dataloader = DataLoader(dataset, sampler=sampler_func, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformers Class is used to load, train, and evaluate the BERT model.\n",
        "\n",
        "class Transformers:\n",
        "    model = None\n",
        "\n",
        "    def __init__(self, tokenizer):\n",
        "        self.pad_token_label_id = PAD_TOKEN_LABEL_ID\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and not NO_CUDA else \"cpu\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            self.load()\n",
        "\n",
        "        embeddings = list(convert_to_embedding([(sentence, -1)]))\n",
        "        preds = self._predict_tags_batched(embeddings)        #predict the sentments is positive or negative\n",
        "        return preds\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        from sklearn.metrics import classification_report\n",
        "        y_pred = self._predict_tags_batched(dataloader)\n",
        "        y_true = np.append(np.zeros(12500), np.ones(12500))\n",
        "\n",
        "        score = classification_report(y_true, y_pred)\n",
        "        print(score)\n",
        "\n",
        "\n",
        "#Batch prediction can improve efficiency and lead to more stable results, especially in scenarios involving gradient descent and backpropagation.\n",
        "    def _predict_tags_batched(self, dataloader):\n",
        "        preds = []\n",
        "        self.model.eval()\n",
        "        for batch in tqdm(dataloader, desc=\"Computing NER tags\"):\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(batch[0])\n",
        "                _, is_neg = torch.max(outputs[0], 1)\n",
        "                preds.extend(is_neg.cpu().detach().numpy())\n",
        "\n",
        "        return preds\n",
        "\n",
        "# traning funciton here is a general fine-tuning method that can be used to fine-tune a loaded BERT model for various different tasks（ text classification, named entity recognition）\n",
        "#It sets up the optimizer, learning rate scheduler, performs gradient accumulation, and trains the entire model.\n",
        "\n",
        "    def train(self, dataloader, model, epochs):\n",
        "        assert self.model is None  # make sure we are not use training model\n",
        "        model.to(self.device)\n",
        "        self.model = model\n",
        "\n",
        "        t_total = len(dataloader) // GRADIENT_ACCUMULATION_STEPS * epochs  #calculate the training step\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": model.bert.parameters(), \"lr\": LEARNING_RATE_MODEL},\n",
        "            {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n",
        "\n",
        "        # Train!\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"Training on %d examples\", len(dataloader))\n",
        "        print(\"Num Epochs = %d\", epochs)\n",
        "        print(\"Total optimization steps = %d\", t_total)\n",
        "\n",
        "        global_step = 0\n",
        "        tr_loss, logging_loss = 0.0, 0.0\n",
        "        model.zero_grad()\n",
        "        train_iterator = trange(epochs, desc=\"Epoch\")\n",
        "        self._set_seed()\n",
        "        for _ in train_iterator:\n",
        "            epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                model.train()\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                outputs = model(batch[0], labels=batch[1]) #feedforward\n",
        "                loss = outputs[0]  #calculate loss, model outputs are always tuple in pytorch-transformers (see doc)\n",
        "\n",
        "                if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "                loss.backward() # calcualte gredient in backpropagation\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    optimizer.step()\n",
        "                    model.zero_grad() #\n",
        "                    global_step += 1\n",
        "\n",
        "        self.model = model\n",
        "        return global_step, tr_loss / global_step\n",
        "\n",
        "    def _set_seed(self):\n",
        "        torch.manual_seed(SEED)\n",
        "        if self.device == 'gpu':\n",
        "            torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "    def load(self, model_dir='weights/'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "        self.model.to(self.device)\n"
      ],
      "metadata": {
        "id": "Xo50buBcuLwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjoVtP7veLqz",
        "outputId": "7ce66b0e-4967-4102-b839-5220a2b9867f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing NER tags: 100%|██████████| 1563/1563 [05:48<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.87      0.86     12500\n",
            "         1.0       0.87      0.85      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# training function is specific to fine-tuning a BERT model for the task of sentiment analysis\n",
        "def train(epochs=20, output_dir=\"weights/\"):\n",
        "    num_labels = 2  # negative and positive reviews\n",
        "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)          #预训练的 BERT 模型是通过 from_pretrained 方法加载的，这意味着该模型已经在一个大型的语料库（如维基百科）上进行了预训练\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)    #选择预训练的 BERT 模型，准备对其进行微调\n",
        "\n",
        "    dataloader = prepare_dataloader(tokenizer, train=True) # prepare data for fine tune\n",
        "    predictor = Transformers(tokenizer)\n",
        "    predictor.train(dataloader, model, epochs) #  model fine tune\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "def evaluate(model_dir=\"weights/\"):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "    dataloader = prepare_dataloader(tokenizer, train=False, sampler=None)\n",
        "    predictor = Transformers(tokenizer)\n",
        "    predictor.load(model_dir=model_dir)\n",
        "    predictor.evaluate(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "path = '/content/gdrive/My Drive/bert/weights/'\n",
        "#os.makedirs(path, exist_ok=True)\n",
        "#train(epochs=10, output_dir=path)\n",
        "evaluate(model_dir=path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thlcE_u4eHhs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}